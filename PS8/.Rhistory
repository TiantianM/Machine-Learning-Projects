library(ggplot2)
library(data.table)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# Load the dataset
dataset <- read.csv("JaneTrain.csv", header = TRUE)
# Get the most common words used by Jane Austen
jane_words <- colSums(dataset[dataset$jane == "JAUSTEN", 2:ncol(dataset)])
jane_sorted <- sort(jane_words, decreasing = TRUE)
top_10_jane <- head(jane_sorted, 10)
# Get the most common words used by H.G. Wells
wells_words <- colSums(dataset[dataset$jane == "HGWELLS", 2:ncol(dataset)])
wells_sorted <- sort(wells_words, decreasing = TRUE)
top_10_wells <- head(wells_sorted, 10)
# Create a table of the top 10 words for Jane Austen
jane_table <- data.frame(word = names(top_10_jane),
count = as.vector(top_10_jane))
# Create a table of the top 10 words for H.G. Wells
wells_table <- data.frame(word = names(top_10_wells),
count = as.vector(top_10_wells))
# Print the tables
print(jane_table)
print(wells_table)
# Train logistic regression classifer
library(caret)
library(MLmetrics)
# 5 fold CV
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
# train the logistic regression model
logistic_mod <- train(jane ~ ., data = dataset, trControl = train_control,
method = "glm", family = binomial(link = "logit"),control = list(maxit = 100))
logistic_mod
# train the logistic regression model
logistic_mod <- train(jane ~ ., data = dataset, trControl = train_control,
method = "glm", family = binomial(link = "logit"),control = list(maxit = 100))
logistic_mod
#  log loss and misclassification rate
log<- c(logistic_mod$results$logLoss)
mis <- 1 - c(logistic_mod$results$Accuracy)
metrics_tab <- cbind(log, mis)
colnames(metrics_tab) <- c("LogLoss", "MisclassRate")
rownames(metrics_tab) <- c("Logistic")
print(metrics_tab)
# coefficients
coe <- coef(logistic_mod$finalModel)
# 10 largest
sort1 <- sort(coe, decreasing = TRUE)
largest10 <- head(sort1, 10)
# 10 smallest
sort2 <- sort(coe, decreasing = FALSE)
smallest10 <- head(sort2, 10)
table1 <- data.frame(word = names(largest10),
largest_coefficients = as.vector(largest10))
table1
table2 <- data.frame(word = names(smallest10),
smallest_coefficients = as.vector(smallest10))
table2
# Load the dataset
dataset <- read.csv("JaneTrain.csv", header = TRUE)
# Get the most common words used by Jane Austen
jane_words <- colSums(dataset[dataset$jane == "JAUSTEN", 2:ncol(dataset)])
jane_sorted <- sort(jane_words, decreasing = TRUE)
top_10_jane <- head(jane_sorted, 10)
# Get the most common words used by H.G. Wells
wells_words <- colSums(dataset[dataset$jane == "HGWELLS", 2:ncol(dataset)])
wells_sorted <- sort(wells_words, decreasing = TRUE)
top_10_wells <- head(wells_sorted, 10)
# Create a table of the top 10 words for Jane Austen
jane_table <- data.frame(word = names(top_10_jane),
count = as.vector(top_10_jane))
# Create a table of the top 10 words for H.G. Wells
wells_table <- data.frame(word = names(top_10_wells),
count = as.vector(top_10_wells))
# Print the tables
print(jane_table)
print(wells_table)
# Train logistic regression classifer
library(caret)
library(MLmetrics)
# 5 fold CV
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
# train the logistic regression model
logistic_mod <- train(jane ~ ., data = dataset, trControl = train_control,
method = "glm", family = binomial(link = "logit"),control = list(maxit = 100))
#  log loss and misclassification rate
log<- c(logistic_mod$results$logLoss)
mis <- 1 - c(logistic_mod$results$Accuracy)
metrics_tab <- cbind(log, mis)
colnames(metrics_tab) <- c("LogLoss", "MisclassRate")
rownames(metrics_tab) <- c("Logistic")
print(metrics_tab)
# coefficients
coe <- coef(logistic_mod$finalModel)
# coefficients
coe <- coef(logistic_mod$finalModel)
# 10 largest
sort1 <- sort(coe, decreasing = TRUE)
largest10 <- head(sort1, 10)
# 10 smallest
sort2 <- sort(coe, decreasing = FALSE)
smallest10 <- head(sort2, 10)
table1 <- data.frame(word = names(largest10),
largest_coefficients = as.vector(largest10))
table1
table2 <- data.frame(word = names(smallest10),
smallest_coefficients = as.vector(smallest10))
table2
lassoModel <- train(jane ~ ., data = dataset, method = "glmnet",family = "binomial", link = "logit", trControl = train_control, tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 1, length = 500)))
lassoModel <- train(jane ~ ., data = dataset, method = "glmnet",family = "binomial", link = "logit", trControl = train_control, tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 1, length = 500)))
# Save the optimal value of lambda
opt_lambda <- lassoModel $ bestTune $lambda
# associated log loss and misclassification rate
lasso_loss <- lassoModel $ results $logLoss [lassoModel$results$lambda == opt_lambda]
lasso_misclass <- 1 - lassoModel$results$Accuracy[lassoModel$results$lambda == opt_lambda]
lasso_loss
lasso_misclass
dataset1 <- read.csv("JaneTrain.csv", header = TRUE)
library(dplyr)
library(glmnet)
train_x <- dataset [, -1]
train_y <- dataset$jane
train_y <- dplyr::recode(dataset1$jane, JAUSTEN = "1", HGWELLS = "0")
lasso_opt <- glmnet(x = train_x, y = train_y, alpha = 1, family = "binomial", link = "logit",lambda = opt_lambda, nfolds = 5)
# Extract the coefficients for the optimal lambda value
lassoCoe <- coef(lasso_opt, s = opt_lambda)
lasso_table<- as.data.frame(as.matrix(lassoCoe))
colnames(lasso_table) <- c( "coefficient")
# Extract the 10 largest regression coefficients
lasso_largest <- lasso_table %>% arrange(desc(coefficient))%>% slice_head(n = 10)
lasso_largest
# Extract the 10 smallest regression coefficients
lasso_smallest <- lasso_table %>% arrange((coefficient))%>% slice_head(n = 10)
lasso_smallest
# train with linear SVC
library(doParallel)
ncores <- detectCores()
cl <- makeCluster(ncores - 2)
registerDoParallel(cl)
svm_linear_mod <- train(jane ~.,
data = dataset, trControl = train_control, method = "svmLinear",
tuneGrid = expand.grid(.C = 10^seq(-5, 5, length = 11)))
# train with linear SVC
library(doParallel)
ncores <- detectCores()
cl <- makeCluster(ncores - 2)
registerDoParallel(cl)
svm_linear_mod <- train(jane ~.,
data = dataset, trControl = train_control, method = "svmLinear",
tuneGrid = expand.grid(.C = 10^seq(-5, 5, length = 11)))
stopCluster(cl)
#svm_linear_mod
# A SVM with a polynomial kernel
ncores <- detectCores()
cl <- makeCluster(ncores - 2)
registerDoParallel(cl)
svm_poly_mod <- train(jane ~ ., data = dataset,
trControl = train_control, method = "svmPoly", tuneGrid = expand.grid(.C = 10^seq(-5,
5, length = 11), .degree = c(2, 3, 4), .scale = c(1)))
# A SVM with a polynomial kernel
ncores <- detectCores()
cl <- makeCluster(ncores - 2)
registerDoParallel(cl)
svm_poly_mod <- train(jane ~ ., data = dataset,
trControl = train_control, method = "svmPoly", tuneGrid = expand.grid(.C = 10^seq(-5,
5, length = 11), .degree = c(2, 3, 4), .scale = c(1)))
