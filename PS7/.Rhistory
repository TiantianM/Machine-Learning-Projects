library(ggplot2)
library(data.table)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
wine_1 <- read.csv("wineTrainCharacter.csv")
# recode
wine <- wine_1 %>% mutate (Class = recode (Class,'Class2'= 'Class0', 'Class3'='Class0'))
# Train logistic regression classifer
train <- wine[, c("Class","Color", "OD280")]
library(caret)
library(MLmetrics)
# 10  fold CV
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE,
summaryFunction = multiClassSummary)
# train the logistic regression model
logistic_mod <- train(Class ~ Color + OD280, data = train,
trControl = train_control, method = "glm", family = binomial(link = "logit"))
# LDA classifier
lda_mod <- train(Class ~ Color + OD280, data = train,
trControl = train_control, method = "lda")
# QDA classifier
qda_mod <- train(Class ~ Color + OD280, data = train,
trControl = train_control, method = "qda")
logloss_tab <- c(logistic_mod$results$logLoss, lda_mod$results$logLoss,
qda_mod$results$logLoss)
misclass_tab <- 1 - c(logistic_mod$results$Accuracy, lda_mod$results$Accuracy,
qda_mod$results$Accuracy)
metrics_tab <- cbind(logloss_tab, misclass_tab)
colnames(metrics_tab) <- c("LogLoss", "MisclassRate")
rownames(metrics_tab) <- c("Logistic", "LDA", "QDA")
print(metrics_tab)
# Create a grid for our features
grid_df <- expand.grid("Color" = seq(1,13,length = 200), "OD280" = seq(1,4,length = 200))
# Generate the Bayes' class for three classifers.
logistic_class_pred <- predict(logistic_mod, newdata = grid_df, type = "raw")
lda_class_pred <- predict(lda_mod, newdata = grid_df, type = "raw")
qda_class_pred <- predict(qda_mod, newdata = grid_df, type = "raw")
# Put them in a data.frame
class_preds <- data.frame(grid_df, logistic_class_pred, lda_class_pred, qda_class_pred)
# draw the plots
p1 <- ggplot(data = train, aes(x = Color, y = OD280, color = Class)) +
geom_point()
p2 <- ggplot(class_preds, aes(x = Color, y = OD280, color = logistic_class_pred)) +
geom_point() + theme(legend.position = "none") + ggtitle("Logistic")
p3 <- ggplot(class_preds, aes(x = Color, y = OD280, color = lda_class_pred)) +
geom_point() + theme(legend.position = "none") + ggtitle("LDA")
p4 <- ggplot(class_preds, aes(x = Color, y = OD280, color = qda_class_pred)) +
geom_point() + theme(legend.position = "none") + ggtitle("QDA")
cowplot::plot_grid(p1, p2, p3,p4)
train2 <- wine_1[, c("Class","Color", "OD280")]
# train the multinomial logistic regression
mnl_mod <- train(Class ~ Color + OD280, data = train2, trControl = train_control, method = "multinom",trace=0,
MaxNWts = 550, tuneGrid = data.frame(.decay = 0))
# LDA classifier
lda_mod2 <- train(Class~ ., data = train2, trControl = train_control, method = "lda")
# QDA classifier
qda_mod2 <- train(Class ~ ., data = train2, trControl = train_control, method = "qda")
# Gaussian marginals
nb_norm_mod2 <- train(Class ~ ., data = train2, trControl = train_control, method = "naive_bayes", tuneGrid = data.frame(.usekernel = FALSE,.laplace = 0, .adjust = 1))
# KDE marginals
nb_kde_mod2 <- train(Class ~ ., data = train2, trControl = train_control, method = "naive_bayes", tuneGrid = data.frame(.usekernel = TRUE, .laplace = 0, .adjust = 1))
logloss_tab2 <- c(mnl_mod$results$logLoss, lda_mod2$results$logLoss,
qda_mod2$results$logLoss, nb_norm_mod2$results$logLoss, nb_kde_mod2$results$logLoss)
misclass_tab2 <- 1 - c(mnl_mod$results$Accuracy, lda_mod2$results$Accuracy,
qda_mod2$results$Accuracy, nb_norm_mod2$results$Accuracy, nb_kde_mod2$results$Accuracy)
metrics_tab2 <- cbind(logloss_tab2, misclass_tab2)
colnames(metrics_tab2) <- c("LogLoss", "MisclassRate")
rownames(metrics_tab2) <- c("Multinom. Logistic", "LDA", "QDA", "NB - Norm",
"NB - KDE")
print(metrics_tab2)
# Generate the Bayes' class for three classifers.
logistic_class_pred2 <- predict(mnl_mod, newdata = grid_df, type = "raw")
lda_class_pred2 <- predict(lda_mod2, newdata = grid_df, type = "raw")
qda_class_pred2 <- predict(qda_mod2, newdata = grid_df, type = "raw")
nb_norm_class_pred <- predict(nb_norm_mod2, newdata = grid_df,type = "raw")
nb_kde_class_pred <- predict(nb_kde_mod2, newdata = grid_df, type = "raw")
# Put them in a data.frame
class_preds2 <- data.frame(grid_df, logistic_class_pred2, lda_class_pred2,
qda_class_pred2, nb_norm_class_pred, nb_kde_class_pred)
# draw the plots
p11 <- ggplot(data = train2, aes(x = Color, y = OD280, color = Class)) +
geom_point()
p22 <- ggplot(class_preds2, aes(x = Color, y = OD280, color = logistic_class_pred2)) +
geom_point() + theme(legend.position = "none") + ggtitle("Multinomial Logistic")
p33 <- ggplot(class_preds2, aes(x = Color, y = OD280, color = lda_class_pred2)) +
geom_point() + theme(legend.position = "none") + ggtitle("LDA")
p44 <- ggplot(class_preds2, aes(x = Color, y = OD280, color = qda_class_pred2)) +
geom_point() + theme(legend.position = "none") + ggtitle("QDA")
p55 <- ggplot(class_preds2, aes(x = Color, y = OD280, color = nb_norm_class_pred)) +
geom_point() + theme(legend.position = "none") + ggtitle("NB - Norm")
p66 <- ggplot(class_preds2, aes(x = Color, y = OD280, color = nb_kde_class_pred)) +
geom_point() + theme(legend.position = "none") + ggtitle("NB - KDE")
cowplot::plot_grid(p11, p22, p33,p44,p55,p66)
train <- read.csv("wineTrainCharacter.csv")
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = multiClassSummary)
# multinomial logistic regression
mnl_mod_p3 <- train(Class ~ ., data = train, trControl = train_control, method = "multinom",trace=0, MaxNWts = 550, tuneGrid = data.frame(.decay = 0))
# lda
lda_mod_p3 <- train(Class ~ ., data = train, trControl = train_control,
method = "lda")
qda_mod_p3 <- train(Class ~ ., data = train, trControl = train_control,
method = "qda")
# usekernel = FALSE for normal marginals
nb_norm_mod_p3 <- train(Class ~ ., data = train, trControl = train_control,
method = "naive_bayes", tuneGrid = data.frame(.usekernel = FALSE,
.laplace = 0, .adjust = 1))
# usekernel = TRUE for KDE marginals
nb_kde_mod_p3 <- train(Class ~ ., data = train, trControl = train_control,
method = "naive_bayes", tuneGrid = data.frame(.usekernel = TRUE,
.laplace = 0, .adjust = 1))
# table summary
logloss_tab_p3 <- c(mnl_mod_p3$results$logLoss, lda_mod_p3$results$logLoss,
qda_mod_p3$results$logLoss, nb_norm_mod_p3$results$logLoss, nb_kde_mod_p3$results$logLoss)
misclass_tab_p3 <- 1 - c(mnl_mod_p3$results$Accuracy, lda_mod_p3$results$Accuracy,
qda_mod_p3$results$Accuracy, nb_norm_mod_p3$results$Accuracy, nb_kde_mod_p3$results$Accuracy)
metrics_tab_p3 <- cbind(logloss_tab_p3, misclass_tab_p3)
colnames(metrics_tab_p3) <- c("LogLoss", "MisclassRate")
rownames(metrics_tab_p3) <- c("Multinom. Logistic", "LDA", "QDA", "NB - Norm",
"NB - KDE")
library(kableExtra)
kable(round(metrics_tab_p3, 3)) %>%
kable_styling("striped") %>%
scroll_box(width = "100%")
# Load the dataset
dataset <- read.csv("JaneTrain.csv", header = TRUE)
# Load the dataset
dataset <- read.csv("JaneTrain.csv", header = TRUE)
# Get the most common words used by Jane Austen
jane_austen_words <- colSums(dataset[dataset$jane == "JAUSTEN", 2:ncol(dataset)])
jane_austen_words_sorted <- sort(jane_austen_words, decreasing = TRUE)
top_10_jane_austen_words <- head(jane_austen_words_sorted, 10)
# Get the most common words used by H.G. Wells
h_g_wells_words <- colSums(dataset[dataset$jane == "HGWELLS", 2:ncol(dataset)])
h_g_wells_words_sorted <- sort(h_g_wells_words, decreasing = TRUE)
top_10_h_g_wells_words <- head(h_g_wells_words_sorted, 10)
# Create a table of the top 10 words for Jane Austen
ja_table <- data.frame(word = names(top_10_jane_austen_words),
count = as.vector(top_10_jane_austen_words))
rownames(ja_table) <- NULL
# Create a table of the top 10 words for H.G. Wells
hw_table <- data.frame(word = names(top_10_h_g_wells_words),
count = as.vector(top_10_h_g_wells_words))
rownames(hw_table) <- NULL
# Add a column to each table showing the number of times each word appears
ja_table$num_appearances <- sapply(ja_table$word, function(x) jane_austen_words[x])
hw_table$num_appearances <- sapply(hw_table$word, function(x) h_g_wells_words[x])
# Print the tables
cat("Top 10 words used by Jane Austen:\n")
print(ja_table)
cat("\n")
cat("Top 10 words used by H.G. Wells:\n")
print(hw_table)
# Load the dataset
dataset <- read.csv("JaneTrain.csv", header = TRUE)
# Get the most common words used by Jane Austen
jane_austen_words <- colSums(dataset[dataset$jane == "JAUSTEN", 2:ncol(dataset)])
jane_austen_words_sorted <- sort(jane_austen_words, decreasing = TRUE)
top_10_jane_austen_words <- head(jane_austen_words_sorted, 10)
# Get the most common words used by H.G. Wells
h_g_wells_words <- colSums(dataset[dataset$jane == "HGWELLS", 2:ncol(dataset)])
h_g_wells_words_sorted <- sort(h_g_wells_words, decreasing = TRUE)
top_10_h_g_wells_words <- head(h_g_wells_words_sorted, 10)
# Create a table of the top 10 words for Jane Austen
ja_table <- data.frame(word = names(top_10_jane_austen_words),
count = as.vector(top_10_jane_austen_words))
rownames(ja_table) <- NULL
# Create a table of the top 10 words for H.G. Wells
hw_table <- data.frame(word = names(top_10_h_g_wells_words),
count = as.vector(top_10_h_g_wells_words))
rownames(hw_table) <- NULL
# Add a column to each table showing the number of times each word appears
ja_table$num_appearances <- sapply(ja_table$word, function(x) jane_austen_words[x])
hw_table$num_appearances <- sapply(hw_table$word, function(x) h_g_wells_words[x])
# Print the tables
print(ja_table)
print(hw_table)
ja_table$num_appearances <- sapply(ja_table$word, function(x) jane_austen_words[x])
hw_table$num_appearances <- sapply(hw_table$word, function(x) h_g_wells_words[x])
# Get the most common words used by Jane Austen
jane_austen_words <- colSums(dataset[dataset$jane == "JAUSTEN", 2:ncol(dataset)])
jane_austen_words_sorted <- sort(jane_austen_words, decreasing = TRUE)
top_10_jane_austen_words <- head(jane_austen_words_sorted, 10)
# Get the most common words used by H.G. Wells
h_g_wells_words <- colSums(dataset[dataset$jane == "HGWELLS", 2:ncol(dataset)])
h_g_wells_words_sorted <- sort(h_g_wells_words, decreasing = TRUE)
top_10_h_g_wells_words <- head(h_g_wells_words_sorted, 10)
# Create a table of the top 10 words for Jane Austen
ja_table <- data.frame(word = names(top_10_jane_austen_words),
count = as.vector(top_10_jane_austen_words))
rownames(ja_table) <- NULL
# Create a table of the top 10 words for H.G. Wells
hw_table <- data.frame(word = names(top_10_h_g_wells_words),
count = as.vector(top_10_h_g_wells_words))
rownames(hw_table) <- NULL
# Add a column to each table showing the number of times each word appears
#ja_table$num_appearances <- sapply(ja_table$word, function(x) jane_austen_words[x])
#hw_table$num_appearances <- sapply(hw_table$word, function(x) h_g_wells_words[x])
# Print the tables
print(ja_table)
print(hw_table)
# Get the most common words used by Jane Austen
jane_austen_words <- colSums(dataset[dataset$jane == "JAUSTEN", 2:ncol(dataset)])
jane_austen_words_sorted <- sort(jane_austen_words, decreasing = TRUE)
top_10_jane_austen_words <- head(jane_austen_words_sorted, 10)
# Get the most common words used by H.G. Wells
h_g_wells_words <- colSums(dataset[dataset$jane == "HGWELLS", 2:ncol(dataset)])
h_g_wells_words_sorted <- sort(h_g_wells_words, decreasing = TRUE)
top_10_h_g_wells_words <- head(h_g_wells_words_sorted, 10)
# Create a table of the top 10 words for Jane Austen
ja_table <- data.frame(word = names(top_10_jane_austen_words),
count = as.vector(top_10_jane_austen_words))
#rownames(ja_table) <- NULL
# Create a table of the top 10 words for H.G. Wells
hw_table <- data.frame(word = names(top_10_h_g_wells_words),
count = as.vector(top_10_h_g_wells_words))
#rownames(hw_table) <- NULL
# Print the tables
print(ja_table)
print(hw_table)
# Get the most common words used by Jane Austen
jane_austen_words <- colSums(dataset[dataset$jane == "JAUSTEN", 2:ncol(dataset)])
jane_austen_words_sorted <- sort(jane_austen_words, decreasing = TRUE)
top_10_jane_austen_words <- head(jane_austen_words_sorted, 10)
# Get the most common words used by H.G. Wells
h_g_wells_words <- colSums(dataset[dataset$jane == "HGWELLS", 2:ncol(dataset)])
h_g_wells_words_sorted <- sort(h_g_wells_words, decreasing = TRUE)
top_10_h_g_wells_words <- head(h_g_wells_words_sorted, 10)
# Create a table of the top 10 words for Jane Austen
ja_table <- data.frame(word = names(top_10_jane_austen_words),
count = as.vector(top_10_jane_austen_words))
# Create a table of the top 10 words for H.G. Wells
hw_table <- data.frame(word = names(top_10_h_g_wells_words),
count = as.vector(top_10_h_g_wells_words))
# Print the tables
print(ja_table)
print(hw_table)
# Get the most common words used by Jane Austen
jane_words <- colSums(dataset[dataset$jane == "JAUSTEN", 2:ncol(dataset)])
jane_sorted <- sort(jane_words, decreasing = TRUE)
top_10_jane <- head(jane_sorted, 10)
# Get the most common words used by H.G. Wells
wells_words <- colSums(dataset[dataset$jane == "HGWELLS", 2:ncol(dataset)])
wells_sorted <- sort(wells_words, decreasing = TRUE)
top_10_wells <- head(wells_sorted, 10)
# Create a table of the top 10 words for Jane Austen
jane_table <- data.frame(word = names(top_10_jane),
count = as.vector(top_10_jane))
# Create a table of the top 10 words for H.G. Wells
wells_table <- data.frame(word = names(top_10_wells),
count = as.vector(top_10_wells))
# Print the tables
print(jane_table)
print(wells_table)
# Load the dataset
dataset <- read.csv("JaneTrain.csv", header = TRUE)
# Get the most common words used by Jane Austen
jane_words <- colSums(dataset[dataset$jane == "JAUSTEN", 2:ncol(dataset)])
jane_sorted <- sort(jane_words, decreasing = TRUE)
top_10_jane <- head(jane_sorted, 10)
# Get the most common words used by H.G. Wells
wells_words <- colSums(dataset[dataset$jane == "HGWELLS", 2:ncol(dataset)])
wells_sorted <- sort(wells_words, decreasing = TRUE)
top_10_wells <- head(wells_sorted, 10)
# Create a table of the top 10 words for Jane Austen
jane_table <- data.frame(word = names(top_10_jane),
count = as.vector(top_10_jane))
# Create a table of the top 10 words for H.G. Wells
wells_table <- data.frame(word = names(top_10_wells),
count = as.vector(top_10_wells))
# Print the tables
print(jane_table)
print(wells_table)
View(dataset)
# Train logistic regression classifer
jane <- dataset[dataset$jane == "JAUSTEN", ]
View(jane)
# Train logistic regression classifer
jane <- dataset[dataset$jane == "JAUSTEN", ]
library(caret)
library(MLmetrics)
# Train logistic regression classifer
jane <- dataset[dataset$jane == "JAUSTEN", ]
library(caret)
library(MLmetrics)
# 5 fold CV
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
wine_1 <- read.csv("wineTrainCharacter.csv")
# recode
wine <- wine_1 %>% mutate (Class = recode (Class,'Class2'= 'Class0', 'Class3'='Class0'))
View(wine_1)
# Train logistic regression classifer
library(caret)
library(MLmetrics)
# 5 fold CV
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
# Load the dataset
dataset <- read.csv("JaneTrain.csv", header = TRUE)
# Get the most common words used by Jane Austen
jane_words <- colSums(dataset[dataset$jane == "JAUSTEN", 2:ncol(dataset)])
jane_sorted <- sort(jane_words, decreasing = TRUE)
top_10_jane <- head(jane_sorted, 10)
# Get the most common words used by H.G. Wells
wells_words <- colSums(dataset[dataset$jane == "HGWELLS", 2:ncol(dataset)])
wells_sorted <- sort(wells_words, decreasing = TRUE)
top_10_wells <- head(wells_sorted, 10)
# Create a table of the top 10 words for Jane Austen
jane_table <- data.frame(word = names(top_10_jane),
count = as.vector(top_10_jane))
# Create a table of the top 10 words for H.G. Wells
wells_table <- data.frame(word = names(top_10_wells),
count = as.vector(top_10_wells))
# Print the tables
print(jane_table)
print(wells_table)
# Train logistic regression classifer
library(caret)
library(MLmetrics)
# 5 fold CV
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
View(dataset)
# train the logistic regression model
logistic_mod <- train(jane ~ ., data = dataset, trControl = train_control,
method = "glm", family = binomial(link = "logit"))
# train the logistic regression model
logistic_mod <- train(jane ~ ., data = dataset, trControl = train_control,
method = "glm", family = binomial(link = "logit"))
# train the logistic regression model
logistic_mod <- train(jane ~ ., data = dataset, trControl = train_control,
method = "glm", family = binomial(link = "logit"),control = list(maxit = 100))
#  log loss and misclassification rate
log <- logistic_mod$results$logLoss
mis <- 1 - logistic_mod$results$Accuracy
#  log loss and misclassification rate
log <- c(logistic_mod$results$logLoss)
mis <- 1 - c(logistic_mod$results$Accuracy)
#  log loss and misclassification rate
log <- c(logistic_mod$results$logLoss)
mis <- 1 - c(logistic_mod$results$Accuracy)
metrics_tab <- cbind(log, mis)
colnames(metrics_tab) <- c("LogLoss", "MisclassRate")
rownames(metrics_tab) <- c("Logistic")
print(metrics_tab)
coe <- coef(logit_mod$finalModel)
coe <- coef(logistic_mod$finalModel)
coe <- coef(logistic_mod$finalModel)
coe
# sort
sort <- sort(coe, decreasing = TRUE)
# sort
sort <- sort(coe, decreasing = TRUE)
sort
# sort
sort <- sort(coe, decreasing = TRUE)
# 10 largest
largest10 <- head(jane_sorted, 10)
largest10
# 10 smallest
# sort
sort <- sort(coe, decreasing = TRUE)
# 10 largest
largest10 <- head(jane_sorted, 10)
sort
# 10 smallest
# sort
sort <- sort(coe, decreasing = TRUE)
# 10 largest
largest10 <- head(sort, 10)
# 10 smallest
# sort
sort <- sort(coe, decreasing = TRUE)
# 10 largest
largest10 <- head(sort, 10)
largest
# sort
sort <- sort(coe, decreasing = TRUE)
# 10 largest
largest10 <- head(sort, 10)
largest10
# 10 smallest
sort1 <- sort(coe, decreasing = TRUE)
# 10 largest
sort1 <- sort(coe, decreasing = TRUE)
largest10 <- head(sort1, 10)
# 10 smallest
sort2 <- sort(coe, decreasing = FALSE)
smallesr10 <- head(sort2, 10)
smallest10
sort1 <- sort(coe, decreasing = TRUE)
# 10 largest
sort1 <- sort(coe, decreasing = TRUE)
largest10 <- head(sort1, 10)
# 10 smallest
sort2 <- sort(coe, decreasing = FALSE)
smallest10 <- head(sort2, 10)
sort1 <- sort(coe, decreasing = TRUE)
# 10 largest
sort1 <- sort(coe, decreasing = TRUE)
largest10 <- head(sort1, 10)
# 10 smallest
sort2 <- sort(coe, decreasing = FALSE)
smallest10 <- head(sort2, 10)
smallest10
# 10 largest
sort1 <- sort(coe, decreasing = TRUE)
largest10 <- head(sort1, 10)
# 10 smallest
sort2 <- sort(coe, decreasing = FALSE)
smallest10 <- head(sort2, 10)
sort1
# 10 largest
sort1 <- sort(coe, decreasing = TRUE)
largest10 <- head(sort1, 10)
# 10 smallest
sort2 <- sort(coe, decreasing = FALSE)
smallest10 <- head(sort2, 10)
smallest10
# 10 largest
sort1 <- sort(coe, decreasing = TRUE)
largest10 <- head(sort1, 10)
# 10 smallest
sort2 <- sort(coe, decreasing = FALSE)
smallest10 <- head(sort2, 10)
table1 <- data.frame(word = names(largest10),
coefficients = as.vector(largest10))
table1 <- data.frame(word = names(largest10),
coefficients = as.vector(largest10))
table1
table1 <- data.frame(word = names(largest10),
largest_coefficients = as.vector(largest10))
table1
table2 <- data.frame(word = names(smallest10),
smallest_coefficients = as.vector(smallest10))
table2
