validationY_sub4 <- dplyr::recode(validationY_sub4$label, "4" = "0", "9" = "1")
# 5 fold CV
train_control4 <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
# Convert label data to a factor variable
trainY_subset4 <- factor(trainY_sub4$label)
trainX_sub4 <- trainX[trainY$label %in% c(4,9), ]
validationX_sub4 <- validationX[validationY$label %in% c(4,9), ]
trainY_sub4 <- trainY[trainY$label %in% c(4,9), ]
validationY_sub4 <- validationY[validationY$label %in% c(4,9), ]
trainX_sub4 <- trainX[trainY$label %in% c(4,9), ]
validationX_sub4 <- validationX[validationY$label %in% c(4,9), ]
trainY_sub4 <- trainY[trainY$label %in% c(4,9), ]
trainY_sub4$label <- gsub("4","0",trainY_sub4$label)
validationY_sub4 <- validationY[validationY$label %in% c(4,9), ]
View(trainY_sub4)
trainX_sub4 <- trainX[trainY$label %in% c(4,9), ]
validationX_sub4 <- validationX[validationY$label %in% c(4,9), ]
trainY_sub4 <- trainY[trainY$label %in% c(4,9), ]
trainY_sub4$label <- gsub("4","0",trainY_sub4$label)
trainY_sub4$label <- gsub("9","1",trainY_sub4$label)
validationY_sub4 <- validationY[validationY$label %in% c(4,9), ]
View(trainY_sub4)
trainX_sub4 <- trainX[trainY$label %in% c(4,9), ]
validationX_sub4 <- validationX[validationY$label %in% c(4,9), ]
trainY_sub4 <- trainY[trainY$label %in% c(4,9), ]
trainY_sub4$label <- gsub("4","0",trainY_sub4$label)
trainY_sub4$label <- gsub("9","1",trainY_sub4$label)
validationY_sub4 <- validationY[validationY$label %in% c(4,9), ]
validationY_sub4$label <- gsub("4","0",validationY_sub4$label)
validationY_sub4$label <- gsub("9","1",validationY_sub4$label)
View(validationY_sub4)
# 5 fold CV
train_control4 <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
# Convert label data to a factor variable
trainY_subset4 <- factor(trainY_sub4$label)
levels(trainY_subset4) <-  make.names(levels(trainY_subset4))
# train the logistic regression model
logistic_mod4 <- train(trainX_sub4, trainY_subset4, trControl = train_control4,
method = "glm", family = binomial(link = "logit"), control = list(maxit = 100))
# log loss and misclassification rate
log4 <- c(logistic_mod4$results$logLoss)
mis4 <- 1 - c(logistic_mod4$results$Accuracy)
metrics_tab4 <- cbind(log4, mis4)
colnames(metrics_tab4) <- c("LogLoss", "MisclassRate")
rownames(metrics_tab4) <- c("Logistic")
print(metrics_tab4)
library(ggplot2)
library(data.table)
library(tidyverse)
library(magrittr)
library(caret)
library(MLmetrics)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
library(ggplot2)
library(data.table)
library(tidyverse)
library(magrittr)
library(caret)
library(MLmetrics)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
library(ggplot2)
library(data.table)
library(tidyverse)
library(magrittr)
library(caret)
library(MLmetrics)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# load the MNIST data
trainX <- read_csv("MNISTTrainXV2.csv")
trainY <- read_csv("MNISTTrainY.csv")
trainYChar <- read_csv("MNISTTrainYChar.csv")
validationX <- read_csv("MNISTValidationX.csv")
validationY <- read_csv("MNISTValidationY.csv")
validationYChar <- read_csv("MNISTValidYChar.csv")
testX <- read_csv("MNISTTestXRand.csv")
testY <- read_csv("MNISTTestYRand.csv")
set.seed(123)
# Create a random sample of 9 image indices
sample_indices <- sample(nrow(trainX), 9)
# Plot the 9 random images using the plot_digit() function
plot_digit <- function(x, bw = FALSE, ...) {
if (sqrt(length(x)) != round(sqrt(length(x)))) {
stop(print("Not a square image! Something is wrong here."))
}
n <- sqrt(length(x))
if (bw == TRUE) {
x <- as.numeric(x > 50) * 256
}
par(pty = "s")
image(matrix(as.matrix(x), nrow = n)[, n:1], col = gray(12:1/12),
...)
}
par(mfrow = c(3, 3))
for (i in 1:9) {
plot_digit(trainX[sample_indices[i], ], main = paste("True Class =", trainY[sample_indices[i],]))
}
# subset training and validation datasets
trainX_sub <- trainX[trainY$label %in% c(0,1), ]
validationX_sub <- validationX[validationY$label %in% c(0,1), ]
trainY_sub <- trainY[trainY$label %in% c(0,1), ]
validationY_sub <- validationY[validationY$label %in% c(0,1), ]
# 5 fold CV
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
# Convert label data to a factor variable
trainY_subset <- factor(trainY_sub$label)
levels(trainY_subset) <-  make.names(levels(trainY_subset))
# train the logistic regression model
logistic_mod <- train(trainX_sub, trainY_subset, trControl = train_control,
method = "glm", family = binomial(link = "logit"), control = list(maxit = 100))
library(ggplot2)
library(data.table)
library(tidyverse)
library(magrittr)
library(caret)
library(MLmetrics)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
library(ggplot2)
library(data.table)
library(tidyverse)
library(magrittr)
library(caret)
library(MLmetrics)
library(glmnet)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# load the MNIST data
trainX <- read_csv("MNISTTrainXV2.csv")
trainY <- read_csv("MNISTTrainY.csv")
trainYChar <- read_csv("MNISTTrainYChar.csv")
validationX <- read_csv("MNISTValidationX.csv")
validationY <- read_csv("MNISTValidationY.csv")
validationYChar <- read_csv("MNISTValidYChar.csv")
testX <- read_csv("MNISTTestXRand.csv")
testY <- read_csv("MNISTTestYRand.csv")
set.seed(123)
# Create a random sample of 9 image indices
sample_indices <- sample(nrow(trainX), 9)
# Plot the 9 random images using the plot_digit() function
plot_digit <- function(x, bw = FALSE, ...) {
if (sqrt(length(x)) != round(sqrt(length(x)))) {
stop(print("Not a square image! Something is wrong here."))
}
n <- sqrt(length(x))
if (bw == TRUE) {
x <- as.numeric(x > 50) * 256
}
par(pty = "s")
image(matrix(as.matrix(x), nrow = n)[, n:1], col = gray(12:1/12),
...)
}
par(mfrow = c(3, 3))
for (i in 1:9) {
plot_digit(trainX[sample_indices[i], ], main = paste("True Class =", trainY[sample_indices[i],]))
}
trainX_sub <- trainX[trainY$label %in% c(0,1), ]
validationX_sub <- validationX[validationY$label %in% c(0,1), ]
trainY_sub <- trainY[trainY$label %in% c(0,1), ]
validationY_sub <- validationY[validationY$label %in% c(0,1), ]
# 5 fold CV
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
# Convert label data to a factor variable
trainY_subset <- factor(trainY_sub$label)
levels(trainY_subset) <-  make.names(levels(trainY_subset))
# train the logistic regression model
logistic_mod <- train(trainX_sub, trainY_subset, trControl = train_control,
method = "glm", family = binomial(link = "logit"), control = list(maxit = 100))
# log loss and misclassification rate
log <- c(logistic_mod$results$logLoss)
mis <- 1 - c(logistic_mod$results$Accuracy)
metrics_tab <- cbind(log, mis)
colnames(metrics_tab) <- c("LogLoss", "MisclassRate")
rownames(metrics_tab) <- c("Logistic")
print(metrics_tab)
# predict on validation set
pred_val <- predict(logistic_mod, validationX_sub)
# create a data frame with actual and predicted labels
val_df <- data.frame(actual = validationY_sub$label, predicted = pred_val)
# identify misclassified images
misclassified <- val_df[val_df$actual != val_df$predicted, ]
misclassified_indices <- rownames(misclassified)
# plot up to 4 misclassified images
par(mfrow = c(2, 2))
for (i in 1:min(4, length(misclassified_indices))) {
index <- as.numeric(misclassified_indices[i])
plot_digit(validationX_sub[index, ], main = paste0("Actual: ", misclassified$actual[i], " Predicted: ", misclassified$predicted[i]))
}
trainX_sub4 <- trainX[trainY$label %in% c(4,9), ]
validationX_sub4 <- validationX[validationY$label %in% c(4,9), ]
trainY_sub4 <- trainY[trainY$label %in% c(4,9), ]
trainY_sub4$label <- gsub("4","0",trainY_sub4$label)
trainY_sub4$label <- gsub("9","1",trainY_sub4$label)
validationY_sub4 <- validationY[validationY$label %in% c(4,9), ]
validationY_sub4$label <- gsub("4","0",validationY_sub4$label)
validationY_sub4$label <- gsub("9","1",validationY_sub4$label)
# 5 fold CV
train_control4 <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
# Convert label data to a factor variable
trainY_subset4 <- factor(trainY_sub4$label)
levels(trainY_subset4) <-  make.names(levels(trainY_subset4))
# train the logistic regression model
logistic_mod4 <- train(trainX_sub4, trainY_subset4, trControl = train_control4,
method = "glm", family = binomial(link = "logit"), control = list(maxit = 100))
# log loss and misclassification rate
log4 <- c(logistic_mod4$results$logLoss)
mis4 <- 1 - c(logistic_mod4$results$Accuracy)
metrics_tab4 <- cbind(log4, mis4)
colnames(metrics_tab4) <- c("LogLoss", "MisclassRate")
rownames(metrics_tab4) <- c("Logistic")
print(metrics_tab4)
log_lasso_cv <- cv.glmnet(x = as.matrix(trainX_sub4), y = trainY_sub4,
family = "binomial", alpha = 1)
View(trainX_sub4)
trainY_sub4$label <- as.numeric(trainY_sub4$label)
log_lasso_cv <- cv.glmnet(x = as.matrix(trainX_sub4), y = trainY_sub4,
family = "binomial", alpha = 1)
summary(trainX_sub4)
summary(trainY_sub4)
trainY_sub4$label <- as.numeric(trainY_sub4$label)
log_lasso_cv <- cv.glmnet(x = as.matrix(trainX_sub4), y = trainY_sub4,
family = "binomial", alpha = 1)
trainY_sub4$label <- as.numeric(trainY_sub4$label)
log_lasso_cv <- cv.glmnet(x = as.matrix(trainX_sub4), y = trainY_sub4,
family = "binomial", alpha = 1)
trainY_sub4$label <- as.numeric(trainY_sub4$label)
t(trainX_sub4)%*%as.matrix(trainX_sub4)
log_lasso_cv <- cv.glmnet(x = as.matrix(trainX_sub4), y = trainY_sub4,
family = "binomial", alpha = 1)
trainY_sub4$label <- as.numeric(trainY_sub4$label)
log_lasso_cv <- cv.glmnet(x = as.matrix(trainX_sub4), y = trainY_sub4,
family = "binomial", alpha = 1)
trainY_sub4 <- as.numeric(trainY_sub4)
trainY_sub4$label <- as.numeric(trainY_sub4$label)
trainX_sub4 <- as.matrix(trainX_sub4)
log_lasso_cv <- cv.glmnet(x = trainX_sub4, y = trainY_sub4,
family = "binomial", alpha = 1)
# Tune the Logistic LASSO classifier model
log_lasso_cv <- cv.glmnet(x = as.matrix(trainX_sub4), y = as.numeric(trainY_sub4$label),
family = "binomial", alpha = 1)
trainX_sub4 <- trainX[trainY$label %in% c(4,9), ]
validationX_sub4 <- validationX[validationY$label %in% c(4,9), ]
trainY_sub4 <- trainY[trainY$label %in% c(4,9), ]
trainY_sub4$label <- gsub("4","0",trainY_sub4$label)
trainY_sub4$label <- gsub("9","1",trainY_sub4$label)
validationY_sub4 <- validationY[validationY$label %in% c(4,9), ]
validationY_sub4$label <- gsub("4","0",validationY_sub4$label)
validationY_sub4$label <- gsub("9","1",validationY_sub4$label)
# Tune the Logistic LASSO classifier model
log_lasso_cv <- cv.glmnet(x = as.matrix(trainX_sub4), y = as.numeric(trainY_sub4$label),
family = "binomial", alpha = 1)
# Get the lambda value
lasso_lambda <- log_lasso_cv$lambda.min
# Now, use caret
logistic_lasso_mod <- train(trainX_sub4, trainY_subset4, trControl = train_control4,
method = "glmnet", family = "binomial", tuneGrid = data.frame(.alpha = 1,
.lambda = lasso_lambda))
# Now, use caret
logistic_lasso_mod <- train(trainX_sub4, trainY_sub4, trControl = train_control4,
method = "glmnet", family = "binomial", tuneGrid = data.frame(.alpha = 1,
.lambda = lasso_lambda))
# Now, use caret
logistic_lasso_mod <- train(trainX_sub4, as.numeric(trainY_sub4$label), trControl = train_control4,
method = "glmnet", family = "binomial", tuneGrid = data.frame(.alpha = 1,
.lambda = lasso_lambda))
# Now, use caret
logistic_lasso_mod <- train(trainX_sub4, trainY_subset4,, trControl = train_control4,
method = "glmnet", family = "binomial", tuneGrid = data.frame(.alpha = 1,
.lambda = lasso_lambda))
log_lasso_loss <- logistic_lasso_mod$results$logLoss
log_lasso_loss <- logistic_lasso_mod$results$logLoss
log_lasso_misclass <- logistic_lasso_mod$results$Accuracy
library(ggplot2)
library(data.table)
library(tidyverse)
library(magrittr)
library(caret)
library(MLmetrics)
library(glmnet)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# load the MNIST data
trainX <- read_csv("MNISTTrainXV2.csv")
trainY <- read_csv("MNISTTrainY.csv")
trainYChar <- read_csv("MNISTTrainYChar.csv")
validationX <- read_csv("MNISTValidationX.csv")
validationY <- read_csv("MNISTValidationY.csv")
validationYChar <- read_csv("MNISTValidYChar.csv")
testX <- read_csv("MNISTTestXRand.csv")
testY <- read_csv("MNISTTestYRand.csv")
# Convert label data to a factor variable
trainY10 <- factor(trainY$label)
levels(trainY10) <-  make.names(levels(trainY10))
# Convert label data to a factor variable
trainY10 <- factor(trainY$label)
levels(trainY10) <-  make.names(levels(trainY10))
# 5 fold CV
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
# Tune the Logistic LASSO classifier model
log_lasso_cv10 <- cv.glmnet(x = as.matrix(trainX), y = as.numeric(trainY$label),
family = "multinomial", alpha = 1)
# 5 fold CV
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
# 5 fold CV
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
# Tune the Logistic LASSO classifier model
log_lasso_cv10 <- cv.glmnet(x = as.matrix(trainX), y = as.numeric(trainY$label),
family = "multinomial", alpha = 1)
# Get the lambda value
lasso_lambda10 <- log_lasso_cv10$lambda.min
start.time <- Sys.time()
# Now, use caret
logistic_lasso_mod10 <- train(trainX, trainY10, trControl = train_control,
method = "glmnet", family = "multinomial", tuneGrid = data.frame(.alpha = 1,
.lambda = lasso_lambda10))
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
set.seed(123)
# Create a random sample of 9 image indices
sample_indices <- sample(nrow(trainX), 9)
# Plot the 9 random images using the plot_digit() function
plot_digit <- function(x, bw = FALSE, ...) {
if (sqrt(length(x)) != round(sqrt(length(x)))) {
stop(print("Not a square image! Something is wrong here."))
}
n <- sqrt(length(x))
if (bw == TRUE) {
x <- as.numeric(x > 50) * 256
}
par(pty = "s")
image(matrix(as.matrix(x), nrow = n)[, n:1], col = gray(12:1/12),
...)
}
par(mfrow = c(3, 3))
for (i in 1:9) {
plot_digit(trainX[sample_indices[i], ], main = paste("True Class =", trainY[sample_indices[i],]))
}
# load the MNIST data
trainX <- read_csv("MNISTTrainXV2.csv")
library(ggplot2)
library(data.table)
library(tidyverse)
library(magrittr)
library(caret)
library(MLmetrics)
library(glmnet)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# load the MNIST data
trainX <- read_csv("MNISTTrainXV2.csv")
trainY <- read_csv("MNISTTrainY.csv")
trainYChar <- read_csv("MNISTTrainYChar.csv")
validationX <- read_csv("MNISTValidationX.csv")
validationY <- read_csv("MNISTValidationY.csv")
validationYChar <- read_csv("MNISTValidYChar.csv")
testX <- read_csv("MNISTTestXRand.csv")
testY <- read_csv("MNISTTestYRand.csv")
trainX_sub4 <- trainX[trainY$label %in% c(4,9), ]
validationX_sub4 <- validationX[validationY$label %in% c(4,9), ]
trainY_sub4 <- trainY[trainY$label %in% c(4,9), ]
trainY_sub4$label <- gsub("4","0",trainY_sub4$label)
trainY_sub4$label <- gsub("9","1",trainY_sub4$label)
validationY_sub4 <- validationY[validationY$label %in% c(4,9), ]
validationY_sub4$label <- gsub("4","0",validationY_sub4$label)
validationY_sub4$label <- gsub("9","1",validationY_sub4$label)
# Load in ranger
library(ranger)
library(knitr)
df = cbind(trainX_sub4, trainY_subset4)
# 5 fold CV
train_control4 <- trainControl(method = "cv", number = 5, classProbs = TRUE,
summaryFunction = multiClassSummary)
# Convert label data to a factor variable
trainY_subset4 <- factor(trainY_sub4$label)
levels(trainY_subset4) <-  make.names(levels(trainY_subset4))
# Load in ranger
library(ranger)
library(knitr)
df = cbind(trainX_sub4, trainY_subset4)
# Convert to a factor
df$trainY_subset4 <- as.factor(df$trainY_subset4)
# Create the factor matrix of outcome indicators
out_indicators <- model.matrix(~trainY_subset4 - 1, df)
# Try 15 values of mtry Create data frame to capture
# outputs
out_df_rf <- data.frame(param = round(seq(1, 36, length = 15)),
oob_error = rep(0, 15), log_loss = rep(0, 15), misclass_rate = rep(0,
15))
for (i in 1:nrow(out_df_rf)) {
rf_mod <- ranger(trainY_subset4 ~ ., data = df, num.trees = 1000,
mtry = out_df_rf$param[i], probability = TRUE, classification = TRUE,
num.threads = 4)
# Get the OOB probability predictions
out_df_rf$oob_error[i] <- rf_mod$prediction.error
# Let's get the log loss Start by getting prob correct
prob_correct <- rowSums(rf_mod$predictions * out_indicators)
# Recode any zeros to second smallest value
prob_correct[prob_correct == 0] <- min(prob_correct[prob_correct !=
0])
out_df_rf$log_loss[i] <- -mean(log(prob_correct))
# Now, get OOB misclass rate Get the column of the max
# class for each row
max_y <- apply(rf_mod$predictions, 1, which.max)
# Translate the number back to the factor levels Of the
# original outcome
max_y <- factor(max_y, labels = levels(df$trainY_subset4))
out_df_rf$misclass_rate[i] <- mean(max_y != df$trainY_subset4)
}
# Try 15 values of mtry Create data frame to capture
# outputs
out_df_rf <- data.frame(param = round(seq(1, 36, length = 15)),
oob_error = rep(0, 15), log_loss = rep(0, 15), misclass_rate = rep(0,
15))
for (i in 1:nrow(out_df_rf)) {
rf_mod <- ranger(trainY_subset4 ~ ., data = df, num.trees = 1000,
mtry = out_df_rf$param[i], probability = TRUE, classification = TRUE,
num.threads = 4)
# Get the OOB probability predictions
out_df_rf$oob_error[i] <- rf_mod$prediction.error
# Let's get the log loss Start by getting prob correct
prob_correct <- rowSums(rf_mod$predictions * out_indicators)
# Recode any zeros to second smallest value
prob_correct[prob_correct == 0] <- min(prob_correct[prob_correct !=
0])
out_df_rf$log_loss[i] <- -mean(log(prob_correct))
# Now, get OOB misclass rate Get the column of the max
# class for each row
max_y <- apply(rf_mod$predictions, 1, which.max)
# Translate the number back to the factor levels Of the
# original outcome
max_y <- factor(max_y, labels = levels(df$trainY_subset4))
out_df_rf$misclass_rate[i] <- mean(max_y != df$trainY_subset4)
}
# Predict
# Generate predictions for test set
preds <- predict(rf_mod, newdata = testX)
# Load in ranger
library(ranger)
library(knitr)
df = cbind(trainX_sub4, trainY_subset4)
df
View(df)
df1 = cbind(testX, testY$label)
# Generate predictions for test set
preds <- predict(rf_mod, newdata = df1)
df1 = cbind(testX, testY$label)
# Convert to a factor
df1$`testY$label` <- as.factor(df1$`testY$label`)
# Generate predictions for test set
preds <- predict(rf_mod, newdata = df1)
# Generate predictions for test set
preds <- predict(rf_mod, newdata = testX)
# Generate predictions for test set
preds <- predict(rf_mod, data = testX)
# Create matrix with image key and predicted class
preds_df <- data.frame(column1 = testY$key, column2 = preds)
names(preds_df) <- c("key", "label")
# Save predictions to CSV file
write.csv(preds_df, "Q4PredictionsMT2.csv", row.names = FALSE)
# Generate predictions for test set
preds <- predict(rf_mod, data = testX)
kable(head(preds$predictions))
max_class_int <- apply(preds$predictions, 1, which.max)
max_class_int <- apply(preds$predictions, 1, which.max)
max_class <- factor(max_class_int, labels = levels(df$trainY_subset4))
head(max_class)
# Create matrix with image key and predicted class
preds_df <- data.frame(column1 = testY$key, column2 = max_class)
names(preds_df) <- c("key", "label")
# Save predictions to CSV file
write.csv(preds_df, "Q4PredictionsMT2.csv", row.names = FALSE)
# Create matrix with image key and predicted class
preds_df <- data.frame(column1 = testY$key, column2 = max_class)
names(preds_df) <- c("key", "label")
preds_df$label <- gsub("X0","4",preds_df$label)
preds_df$label <- gsub("X1","9",preds_df$label)
# Save predictions to CSV file
write.csv(preds_df, "Q4PredictionsMT2.csv", row.names = FALSE)
max_class_int <- apply(preds$predictions, 1, which.max)
max_class <- factor(max_class_int, labels = levels(trainY$label))
max_class_int <- apply(preds$predictions, 1, which.max)
max_class <- factor(max_class_int, labels = levels(df$trainY_subset4))
head(max_class)
# Create matrix with image key and predicted class
preds_df <- data.frame(column1 = testY$key, column2 = max_class)
names(preds_df) <- c("key", "label")
preds_df$label <- gsub("X0","4",preds_df$label)
preds_df$label <- gsub("X1","9",preds_df$label)
# Save predictions to CSV file
write.csv(preds_df, "Q4PredictionsMT2.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
install.packages("MASS")
install.packages("MASS")
library(MASS)
data(biopsy)
data(biopsy)
str(biopsy)
install.packages("MASS")
install.packages("MASS")
install.packages("factoextra")
install.packages("ggfortify")
install.packages("factoextra")
library(MASS)
data(biopsy)
biopsy <- data(biopsy)
data(biopsy)
data(biopsy)
str(biopsy)
